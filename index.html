<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Spiderverse Shaders</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" type="image/png" href="assets/logo.png">
    <link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>

    <style>

        body {
          background-color: #ebebeb;
          margin: 0;
          font-family: Arial;
          line-height: 25px;
        }

        .subtitle {
            font-size: 22px;
        }

        a {
            display: block;
            margin-top: 10px;
            margin-left: 10px;
        }

        pre {
            background-color: #ebebeb;
            padding: 10px;
            margin: 0;
            margin-top: 10px;
            margin-bottom: 5px;
            font-size: 14px;
            word-wrap: break-word;
        }

        img {
            margin-top: 20px;
            margin-bottom: 0px;
            width: 100%;
        }

        .caption {
            font-size: 14px;
            margin-bottom: 20px;
            line-height: 14px;
        }

        .bubble {
            display: inline-block;
            text-align: center;
            width: 100%;
        }

        .bubble > img {
            display: inline-block;
            text-align: center;
            width: 150px;
            margin-right: 20px;
        }

        cmt {
            color: green;
        }

        val {
            background-color: #ebebeb;
            font-family: monospace;
            font-size: 14px;
            padding: 5px;
        }

        .file {
            background-color: #dbdbdb;
            padding: 2px;
            padding-left: 10px;
            padding-right: 10px;
            margin-bottom: 10px;
        }

        .block {
            width:70%; 
            max-width: 860px; 
            margin-left: auto; 
            margin-right: auto;
            background-color: white; 
            padding-top: 30px; 
            padding-left: 20px; 
            padding-right: 20px; 
            padding-bottom: 20px;
            margin-bottom: 20px;
        }

    </style>

    <div class=block>

    
    <div class=subtitle>Spider-Verse: Beyond the Spider-Shaders</div>
    <p>Stylistic NPR Shaders for Everyday Video.</p>

    <div class="bubble">
        <img src="assets/mini_01.gif">
        <img src="assets/mini_04.gif">
        <img src="assets/mini_02.gif">
        <img src="assets/mini_05.gif">
        <img src="assets/mini_03.gif">
        <img src="assets/mini_06.gif">
        <img src="assets/mini_07.gif">
    </div>

    </div>

    <div class=block>


    <div class=subtitle>Table Of Contents</div>

    <a href=#background>Background</a>
    <a href=#approach-paper-expand>Approach [Paper Expand]</a>
    <a href=#experiments-paper-expand>Experiments [Paper Expand]</a>
    <a href=#smearing>Approach [Frame Smearing]</a>
    <a href=#blur>Approach [Directional Motion Blur]</a>
    <a href=#dithering>Approach [Dithering]</a>
    <a href=#rgb-glitch>Approach [RGB Glitch]</a>    
    <a href=#responsibilities>Responsibilities</a>

    </div>

    <div class=block>
    <div class=subtitle id=background>Background</div>
    <p>
        The Spiderverse series is ubiquitous in both the computer graphics and entertainment industry for sparking interest in non-photorealistic rendering. Yet the common question a lot of creators ask is “how can I capture the Spiderverse effect in my animations” or “how can I make my live-action video look like Spiderverse”. In this paper, we aim to write a set of shaders, known as ‘SpiderShaders’ for which draw inspiration from the SpiderVerse style that any artist or videographer can use on their animation or footage. Specifically, this paper investigates three specific effects: motion blur, RGB glitch, and paper expand.
    </p>
    <p>
        Since we aim to create shaders that can be applied to any video footage, whether animation or real-life, we limit our inputs to just the 2D video footage. While many of the original Spiderverse effects were done in 3D, we cannot assume every editor has time to construct an accurate depth map of their footage, so our shader aims to be as accurate as possible while limited to 2D. Additionally, the motion blur effect also requires the optical flow map, which most video editing software can compute nowadays via sparse/dense tracking. We will not be writing an optical flow compute node in our work, but will rather be using footage found online with relevant optical flow data. The outputs will be a stylized 2D footage result depending on the shader used.
    </p>
    <p>
        Our constraints are that our shaders must run on conventional laptop hardware. We demonstrate that most of our shaders can run as After Effects plug-ins and scripts, ensuring that devices that meet the minimum hardware requirements of After Effects can run our filters without substantial time or compute resources. We do not provide a specific performance goal, but rather aim to minimize the compute time as much as possible. As we will discuss later in the paper, one key way of doing so is keeping memory accesses to a minimum, as most of our shaders are memory-bound.
    </p>
    <p>
        Our goal is to be able to faithfully reconstruct many of the Spiderverse filters in a performant way that can be operated on lower-dimensional 2D footage without substantial loss of quality. The hard part of this project is in how open-ended it is. Because there is no benchmark or testing suite, we have to create our own metric of how well we did. The details of this metric will be presented below.
    </p>
    </div>

    <div class=block>
    <div class=subtitle id=approach-paper-expand>Approach [Paper Expand]</div>

    <img src="assets/hobbes_intro.png">
    <div class=caption>Fig 1. Introduction of Hobbes | Across the Spiderverse (2023)</div>

    <p>
        The paper expand effect, in its most basic form, is a course-resolution (few points) expand filter around the edges used to give a punk-rock style in SpiderVerse (Fig 1). The input to this filter is a video of a masked object, which can be provided in one of two ways: as a mask (vector) or as a rotoscope (raster) (Fig 2). 
    </p>

    <img src="assets/rotoscoped.png">
    <div class=caption>Fig 2. Rotoscope [Left] and Mask [Right] | Across the Spiderverse (2023)</div>

    <p>
        A mask is a lot more trivial to work with: sample a random (ideally sparse) set of points along the mask, extrude them via normal approximation of neighboring points, and create a shape layer from the resulting points. Yet most videographers do not use masks for video because masks track and adapt very poorly to non-rigid deformations such as head and body turns. We instead focus on rotoscoped footage for this filter.  
    </p>

    <pre>
    pts = init(); <cmt>// Walk comp bbox, evenly create N points</cmt>
    for(f=0; f&ltnFrames; f++) {
        <cmt>// Subdivide consecutive points that are far</cmt>
        <cmt>// Simplify (merge) consecutive points that are too close</cmt>
        remesh(pts);
        <cmt>// While point in raster, take a step backwards</cmt>
        step_back(pts);
        <cmt>// While point not in raster, take a step forward</cmt>
        <cmt>// On termination, revert a step</cmt>
        step_forward(pts);
        <cmt>// Construct a keyframed shape object out of the points</cmt>
        generate(pts);
    }</pre>
    <div class=caption>Alg 1. Paper Cutout Loop</div>

    <p>
        Our algorithm consists of an initialization step followed by 4 steps repeated per frame. During our remeshing operation, we subdivide and simplify points if they are greater or less than a constant proportional to the composition area. When the algorithm steps forward or backwards, the step size is also relative to the composition area so that different resolutions terminate in approximately the same steps. Stepping forward uses a decaying step size.
    </p>
    <p>
        We implement the filter via After Effect’s scripting language JSX (javascript expressions). Our filter requires periodic lookups into the raster mask, but because JSX does not have any way to access layer pixels, we instead create a text later and schedule a pixel lookup via the layer expression’s built in sampleImage function, reporting the value back as a text value that can be read in by JSX.
    </p>
    <p>
        JSX cannot be linked with other libraries or frameworks, so we were unable to try any loop-level parallelism.  
    </p>
    </div>

    <div class=block>
    <div class=subtitle id=experiments-paper-expand>Experiments [Paper Expand]</div>

    <p>
        We parameterize the filter with the following values:
        <ul>
            <li>Points: number of points. Can change with remeshing step</li>
            <li>Iterations: number of iterations Step Forward</li>
            <li>Frameskip: paper effect is recomputed every x frames</li>
            <li>Radius: the 'thickness' of the paper around the subject</li>
        </ul>
    </p>
    <p>
        Script v4 introduces the remeshing step as well as point re-use between frames, while script v1 initializes and optimizes a new point list each iteration. In benchmarking performance (Tab 1), we find that increasing the number of points does not affect v4 as much, mainly due to point re-use. Increasing the resolution also does not affect performance for both scripts since step sizes are proportional to composition area. Interestingly enough, changing the radius does not affect performance much for both scripts. This is because the sampleImage function used to access memory supports a radius parameter, so we can still execute calls to our paper extend effect with different radius parameters and still make the same number of calls to sampleImage. Calling sampleImage with a radius of 1 or a radius of 100 does not make much of a performance difference to After Effects, and it shows in our results. 
    </p>

    <img src="assets/paper_table.png">
    <div class=caption>Tab 1. Performance of paper cutout averaged over 5 trials</div>

    <img src="assets/spiderman_01.gif">
    <div class=caption>Res 1. Hobbes With paper overlay #1 | Across the Spiderverse (2023)</div>
    <img src="assets/spiderman_02.gif">
    <div class=caption>Res 2. Hobbes With paper overlay #2 | Across the Spiderverse (2023)</div>

    <p>
        We ran our effect on several pieces of footage, two from Spiderverse (Res 1. & 2.) and two live-action (Res 3. & 4.). We rotoscoped the object of interest in After Effects and ran the v4 script. In most cases, we found that reducing the framerate to 12fps (animating on 2s) produced more interesting stylized results.
    </p>
    
    <img src="assets/dancing.gif">
    <div class=caption>Res 3. Full-body rapid movement subjects</div>

        For (Res 4.) we experimented with how the paper extend effect can be used in the VFX pipeline. By combining it with color-burn grunge textures, hue offsets, and exaggerated edge filters all confined to the paper bounds, we are able to create even more stylistic effects on real-life footage that better replicate the SpiderVerse NPR effect.

    <img src="assets/suit.gif">
    <div class=caption>Res 4. Paper expand [Left] and full composite [Right]</div>

    </div>

    <div class=block>
    <div class=subtitle id=#smearing>Approach [Frame Smearing]</div>

    <img src="assets/frame_smear_videos/spiderverse_smear_example.jpeg">
    <div class=caption>Fig 1. Frame Smearing Example | Across the Spiderverse (2023)</div>

    <p>
        The frame smearing plugin aims to create an artistic "smearing" effect which interpolates consecutive frames of movement and creates more movement in the composition. This is better illustrated in the example taken from Spiderverse. There are multiple approaches to achieve this.
    </p>
    <p>
        For one, the shader would need to keep track of the key features that are moving across frames. Then, we would need to extract those key features across a set of consecutive frames, and render them on the current frame. Given that we are limited to 2D footage, this task becomes challenging, as we have no access to 3D vertices that we can extract displacement information from. Moreover, since we can’t pass information between frames in the AE plugin, we decide not to obtain the smearing effect by extracting the moving features from following frames. 
    </p>
    
    <img src="assets/frame_smear_videos/test_frame_smear_1.gif">
    <img src="assets/frame_smear_videos/test_frame_smear_1_dense.gif">
    <div class=caption>Fig 2. Optical Flow Map Example | Tom and Jerry (2022)</div>

    <p>
        Instead, we employ a simplified approach using optical flow. Our workflow is segmented into two steps. First, we run the input video through our optical flow map generator script. This script utilizes the OpenCV API to perform dense optical flow calculations (Gunnar-Farneback method), obtaining velocity vectors illustrating movement between frames. This script produces a new video of the same length, depicting velocity vectors between each frame in HSV, where hue represents the angle of direction and value represents the magnitude of direction. Above is an example of the optical flow map video. We also employ a combination of gaussian and median filtering to denoise this output video. 
    </p>
    <p>
        Next, we insert this generated optical flow map video as an additional layer in After Effects to be inputted into the plugin script (analogous to passing a second texture to the shader). The shader in the plugin script will determine the magnitude and direction of motion at any given pixel (given from the optical flow texture) and will subsequently perform an averaging of pixels in the magnitude and direction of motion. The algorithm looks like the following: 
    </p>
    <img src="assets/frame_smear_videos/smear_algo.png">
    <img src="assets/frame_smear_videos/smear_ae_parameters.png">
    <div class=caption>Fig 2. Algorithm and AE Parameter Sliders</div>
        
    <p>
        Given a pixel's velocity vector's magnitude (derived from optical flow map) is over a threshold constant, we perform averaging over a number of sample points, each point obtained by solving the equation of motion: (x_2, y_2) = (x_1, y_2) s * v * t, where s denotes the scale parameter, v denotes the direction vector obtained from the optical flow map, and t denotes the reciprocal of the number of samples. We parameterize the scale of motion, number of samples, and velocity threshold into AE sliders such that compositors can easily change and tune the effect to their liking. Number of samples determines how many times we observe the smear lines. Scale determines how spaced apart the smear lines are. Threshold determines over which minimum velocity should smear lines be rendered. For instance, setting threshold at 1 would produce no motion blur, while setting threshold to 0 would produce all motion blur as indicated by the optical flow texture. 
    </p>
    <p>
        Fortunately, this shader algorithm runs in real-time and responds instantly to changes in parameterization (for which we cap each parameter at a fixed range for real-time performance), which is what we aim for. This way, artists can preview their composites as soon as they add the effect. We also demonstrate that this plugin can be used over any footage, animation or real-world. Below is a gallery of our carefully-tuned frame smear composites. As motion in Spiderverse is animated every other frame, the Spiderverse footage composites yielded very visually interesting results, namely this “popping” effect when the smear is activated every other frame.  
    </p>
    </div>

    <div class=block>
    <div class=subtitle id=#blur>Approach [Directional Motion Blur]</div>

    <img src="assets/motion_blur_videos/spiderverse_blur_example.jpeg">
    <div class=caption>Fig 1. Motion Blur Example | Across the Spiderverse (2023)</div>
    <p>
        The directional motion blur plugin aims to create the stylistic Spiderverse blur, a comic-like blur that uses streaks of lines along the intended blur direction to accentuate motion. For this effect, we use two shader passes, one to produce blur in a specified direction and the other to draw randomly spaced red and black lines in this specified direction. The algorithm looks like the following: 
    </p>
    <p>
        To achieve directional blur in the first pass, we perform averaging over a number of sample points, each point obtained by solving the following motion equation. *t. We parameterize the number of samples, scale of motion, the magnitude and velocity of the direction vector. Number of samples determines how many samples we average over to obtain the blur. Scale determines how spaced apart the blur lines are. Direction is parameterized in angle (0 to 360 degrees) which allows control over the direction of the blur. 
    </p>
    <p>
        In the second pass, we generate a random set of lines to rasterize. To do so, we iterate over the total number of lines, and in each iteration we seed random numbers to randomize the starting position of the line and the color of the line (red or black). We then determine whether the current pixel is within the line by computing the closest point on the line to the current pixel (projection using dot product). If the distance from the current pixel to the closest point on the line is smaller than the line width, we color that pixel as red or black. To create random lines at each frame, we seed the random numbers using the frame time variable. We utilize the Mersenne Twister pseudo-random number generator from the C++ random library to obtain the random numbers. We also parameterize the number of lines, and line length using AE sliders. 
    </p>
    <p>
        Of course, this second pass is extremely inefficient, given that we run two loops over every single pixel in each frame. We optimize this by combining both passes into one pass. We use one loop to perform both blurring and random line drawing. The combined loop is shown below. We benchmark performance of both algorithms testing a varying number of lines.   
    </p>
    <p>
        Even with the optimization, this shader is still inefficient and not real time. A more obvious approach to this problem is probably not using a shader to generate lines (which performs pixel by pixel computations per frame) but rather using a simple rasterizer that just draws the lines on each frame. 
    </p>
    <p>
        Our directional blur shader is rudimentary. Some additional improvements we could expand our shader on include enabling a perspective 3D blur. For instance, if we have a subject that is moving towards the camera, the direction vectors from different parts of the frame will look different, hence we cannot use the uni-directional motion blur we made for this project. Another potential improvement is anti-aliasing the lines or adding more natural lines or soft contours, rather than a harsh line. A 2D gaussian splatting approach instead of line-drawing could be interesting to investigate and could create more natural looking streaks. Below is a gallery of our custom-tuned directional motion blur composites. 
    </p>
    </div>


    
    <div class=block>
    <div class=subtitle id=#dithering>Approach [Dithering]</div>

    <img src="assets/dither.gif">
    <div class=caption>Fig 1. Shadertoy sample footage using dithering shader</div>

    <p>
        The dithering script uses a fixed Bayer 3x3 matrix for each frame and off-colors each pixel region (currently each 3x3 pixel square in a frame through modulation) to create a dithering pattern. This process is does not use previous frames to inform current frames, as moving dithering patterns could stray from the intended effect. The severity of the coloring can be increased or decreased by modifying an levels variable which essentially alters the number of colors available for that region's palette, moving pixel colorations to the nearest available option after applying the dithering matrix. At a high levels value, the produced frames of the video become visually identical to the original, while low level values produce more distinguishable differentiated dithering regions.
    </p>
    </div>



    <div class=block>
    <div class=subtitle id=#rgb-glitch>Approach [RGB Glitch]</div>

    <img src="assets/negative.gif">
    <div class=caption>Fig 1. Shadertoy sample footage using Mr. Negative Shader</div>
    <img src="assets/rgbnegative.gif">
    <div class=caption>Fig 2. Shadertoy sample footage using Mr. Negative Shader without grayscaling</div>
    <p>
        When first working on the rgb glitch effect, the goal was to emulate the color offput present in Into the Spiderverse where segments of a frame would be offset from their original positions very briefly. The first attempt at this effect resulted in the above effect resembling Mr. Negative from the Spider Man PS4 game. The effect is achieved for each pixel by sampling another pixel a small distance to the left (R), right (G), and above a the pixel (B), then finding the color difference between the offset pixels and the current one. If there is no or little difference (generally non edges), then the pixel coloration will be black, where depending on edge orientation, the edges create a rainbow effect since the colorations are based on three separate edge detection distances individually. By applying a dot product to the resulting color values, this effect can grayscale the frame to create something more resembling Mr. Negative. A benefit of using this particular offset effect feature using individual directions for each color input is that the output edges appear more distinct and uniform (solid colors) while still having a "glitch" effect from pixels nearby edges having diminished offset colorations extending from the edges themselves. The result is a frame where a subject may look as if there are multiple frames stacked on top of each other, creating the intended glitch effect. However, this effect does not use randomization, so this shader was left as-is as a bonus shader,  helping to learn about making the final intended effect.
    </p>
        
    <img src="assets/singlitch.gif">
    <div class=caption>Fig 3. Shadertoy sample footage using Sin Wave Glitch Shader</div> 
    <p>
        This was another attempt at creating a glitch effect, this time using sin waves. The process calculates a "glitch amount" value calulated via a sum of sin values using each pixel's x and y uv values as inputs along with a similar offset strategy to the Mr. Negative shader. The result is several noticable sin waves offsetting the image in a unique rgb effect, but the pattern seemed too regular and thus not "glitchy" enough.
    </p>

    <img src="assets/glitch.gif">
    <div class=caption>Fig 4. Shadertoy sample footage using final Glitch Shader</div> 
    <p>
        This shader uses an existing shader made for creating a glitch effect on images, but adapts it so that it can present multiple frames in a video format. It calculates the edge intensity using a Sobel filter and uses this intensity to modulate the distortion, making glitches more prominent along edges. Additionally, it uses multiple samples found through a series of random offsets, averaging the samples to produce each pixel's final value. Like with each of the shaders made in Shadertoy for this project, they read from memory often and fail to run in real time when used in After Effects (taking a couple miliseconds per frame). The code was exclusively tested in Shadertoy before being used on different footage from the provided sample videos outside of Shadertoy, so if this project was continued, we could optimize these shaders more effectively and avoid reading from memory as often.
    </p>
    </div>


    
    <div class=block>
        <div class=subtitle id=#finalevaluation>Final Evaluation</div>

        <img src="assets/final.gif">
        <div class=caption>Fig 1. Compositing all filters | Across the Spiderverse (2023)</div>

        <p>
            For the final evaluation, we felt that because this is a creative product, that it requires creative evaluation. We interviewed people in the animation industry, from students to graphics engineers to animators, on their thoughts when viewing our results. Some feedback we recieved:
            <br><br>
            <b>Graphics Engineer</b>: looks cool, would love to see more about the robustness.<br>
            <b>After Effects User #1</b>: really cool, can we ship some of these effects in After Effects as a plugin?<br>
            <b>After Effects User #2</b>: I love how quick and easy it is to pick up these effects. The beauty is in the parameteribility<br>
            <b>2D Animator</b>: The effects were pretty interesting. I could see them being used for an abstract atmosphere around a given character. I could use these to make expressive character introductions. <br>
            <b>Artist</b>: I would use them when a character has vigorous motion, good job of highlighting sweepign motion. Style is very specific, so I wouldn't use these for much outside this aesthetic. It looks pretty cool. <br>
            <b>Indie Game Developer</b>: Jarring effect in a stylistically interesting way, matches the style of shows like Smiling Friends. This might feel out of place for anything smooth or grounded with hyper realistic goals like Pixar. They're pretty cool, I had fun checking these out. <br>
        </p>
    </div>



    
    <div class=block>
        <div class=subtitle id=responsibilities>Responsibilities</div>
        <p>
            Oscar Dadfar (odadfar):
            <ul>
                <li>Wrote "Paper Expand" as an After Effects script</li>
                <li>Ran "Paper Expand" tests and VFX compositing example</li>
                <li>Prepared After Effects layer sampling plugin code for starting VFX shaders for teammates</li>
                <li>Ported RGB glitch shader to After Effects as a plugin</li>
                <li>Wrote website template (what you're seeing here)</li>
            </ul>
        </p>
        <p>
            Olivia Loh (olivia77):
            <ul>
                <li>Wrote "Frame Smear" as an After Effects plugin</li>
                <li>Wrote "Motion Blur" as an After Effects plugin</li>
                <li>Wrote optical flow layer generator for "Frame Smear" plugin</li> 
                <li>Ran "Frame Smear" and "Motion Blur" tests and VFX compositing example</li>
            </ul>
        </p>
        <p>
            Maxton Huff (maxton):
            <ul>
                <li>Wrote "Dithering" shader script in Shadertoy</li>
                <li>Wrote "Mr. Negative" shader script in Shadertoy</li>
                <li>Wrote "Sin Wave Glitch" shader script in Shadertoy, unused for final RGB Glitch</li>
                <li>Wrote "RGB Glitch" shader script in Shadertoy</li>
            </ul>
        </p>
    </div>

</body>
</html>
