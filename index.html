<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Spiderverse Shaders</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" type="image/png" href="assets/logo.png">
    <link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>

    <style>

        body {
          background-color: #ebebeb;
          margin: 0;
          font-family: Arial;
          line-height: 25px;
        }

        .subtitle {
            font-size: 22px;
        }

        a {
            display: block;
            margin-top: 10px;
            margin-left: 10px;
        }

        pre {
            background-color: #ebebeb;
            padding: 10px;
            margin: 0;
            margin-top: 10px;
            margin-bottom: 5px;
            font-size: 14px;
            word-wrap: break-word;
        }

        img {
            margin-top: 20px;
            margin-bottom: 0px;
            width: 100%;
        }

        .caption {
            font-size: 14px;
            margin-bottom: 20px;
            line-height: 14px;
        }

        .bubble {
            display: inline-block;
            text-align: center;
            width: 100%;
        }

        .bubble > img {
            display: inline-block;
            text-align: center;
            width: 150px;
            margin-right: 20px;
        }

        cmt {
            color: green;
        }

        val {
            background-color: #ebebeb;
            font-family: monospace;
            font-size: 14px;
            padding: 5px;
        }

        .file {
            background-color: #dbdbdb;
            padding: 2px;
            padding-left: 10px;
            padding-right: 10px;
            margin-bottom: 10px;
        }

        .block {
            width:70%; 
            max-width: 860px; 
            margin-left: auto; 
            margin-right: auto;
            background-color: white; 
            padding-top: 30px; 
            padding-left: 20px; 
            padding-right: 20px; 
            padding-bottom: 20px;
            margin-bottom: 20px;
        }

    </style>

    <div class=block>

    
    <div class=subtitle>Spider-Verse: Beyond the Spider-Shaders</div>
    <p>Stylistic NPR Shaders for Everyday Video.</p>

    <div class="bubble">
        <img src="assets/mini_01.gif">
        <img src="assets/mini_04.gif">
        <img src="assets/mini_02.gif">
        <img src="assets/mini_05.gif">
        <img src="assets/mini_03.gif">
        <img src="assets/mini_06.gif">
        <img src="assets/mini_07.gif">
    </div>

    </div>

    <div class=block>


    <div class=subtitle>Table Of Contents</div>

    <a href=#background>Background</a>
    <a href=#approach-paper-expand>Approach [Paper Expand]</a>
    <a href=#experiments-paper-expand>Experiments [Paper Expand]</a>
    <a href=#smearing>Approach [Frame Smearing]</a>
    <a href=#dithering>Approach [Dithering]</a>
    <a href=#responsibilities>Responsibilities</a>

    </div>

    <div class=block>
    <div class=subtitle id=background>Background</div>
    <p>
        The Spiderverse series is ubiquitous in both the computer graphics and entertainment industry for sparking interest in non-photorealistic rendering. Yet the common question a lot of creators ask is “how can I capture the Spiderverse effect in my animations” or “how can I make my live-action video look like Spiderverse”. In this paper, we aim to write a set of shaders, known as ‘SpiderShaders’ for which draw inspiration from the SpiderVerse style that any artist or videographer can use on their animation or footage. Specifically, this paper investigates three specific effects: motion blur, RGB glitch, and paper expand.
    </p>
    <p>
        Since we aim to create shaders that can be applied to any video footage, whether animation or real-life, we limit our inputs to just the 2D video footage. While many of the original Spiderverse effects were done in 3D, we cannot assume every editor has time to construct an accurate depth map of their footage, so our shader aims to be as accurate as possible while limited to 2D. Additionally, the motion blur effect also requires the optical flow map, which most video editing software can compute nowadays via sparse/dense tracking. We will not be writing an optical flow compute node in our work, but will rather be using footage found online with relevant optical flow data. The outputs will be a stylized 2D footage result depending on the shader used.
    </p>
    <p>
        Our constraints are that our shaders must run on conventional laptop hardware. We demonstrate that most of our shaders can run as After Effects plug-ins and scripts, ensuring that devices that meet the minimum hardware requirements of After Effects can run our filters without substantial time or compute resources. We do not provide a specific performance goal, but rather aim to minimize the compute time as much as possible. As we will discuss later in the paper, one key way of doing so is keeping memory accesses to a minimum, as most of our shaders are memory-bound.
    </p>
    <p>
        Our goal is to be able to faithfully reconstruct many of the Spiderverse filters in a performant way that can be operated on lower-dimensional 2D footage without substantial loss of quality. The hard part of this project is in how open-ended it is. Because there is no benchmark or testing suite, we have to create our own metric of how well we did. The details of this metric will be presented below.
    </p>
    </div>

    <div class=block>
    <div class=subtitle id=approach-paper-expand>Approach [Paper Expand]</div>

    <img src="assets/hobbes_intro.png">
    <div class=caption>Fig 1. Introduction of Hobbes | Across the Spiderverse (2023)</div>

    <p>
        The paper expand effect, in its most basic form, is a course-resolution (few points) expand filter around the edges used to give a punk-rock style in SpiderVerse (Fig 1). The input to this filter is a video of a masked object, which can be provided in one of two ways: as a mask (vector) or as a rotoscope (raster) (Fig 2). 
    </p>

    <img src="assets/rotoscoped.png">
    <div class=caption>Fig 2. Rotoscope [Left] and Mask [Right] | Across the Spiderverse (2023)</div>

    <p>
        A mask is a lot more trivial to work with: sample a random (ideally sparse) set of points along the mask, extrude them via normal approximation of neighboring points, and create a shape layer from the resulting points. Yet most videographers do not use masks for video because masks track and adapt very poorly to non-rigid deformations such as head and body turns. We instead focus on rotoscoped footage for this filter.  
    </p>

    <pre>
    pts = init(); <cmt>// Walk comp bbox, evenly create N points</cmt>
    for(f=0; f&ltnFrames; f++) {
        <cmt>// Subdivide consecutive points that are far</cmt>
        <cmt>// Simplify (merge) consecutive points that are too close</cmt>
        remesh(pts);
        <cmt>// While point in raster, take a step backwards</cmt>
        step_back(pts);
        <cmt>// While point not in raster, take a step forward</cmt>
        <cmt>// On termination, revert a step</cmt>
        step_forward(pts);
        <cmt>// Construct a keyframed shape object out of the points</cmt>
        generate(pts);
    }</pre>
    <div class=caption>Alg 1. Paper Cutout Loop</div>

    <p>
        Our algorithm consists of an initialization step followed by 4 steps repeated per frame. During our remeshing operation, we subdivide and simplify points if they are greater or less than a constant proportional to the composition area. When the algorithm steps forward or backwards, the step size is also relative to the composition area so that different resolutions terminate in approximately the same steps. Stepping forward uses a decaying step size.
    </p>
    <p>
        We implement the filter via After Effect’s scripting language JSX (javascript expressions). Our filter requires periodic lookups into the raster mask, but because JSX does not have any way to access layer pixels, we instead create a text later and schedule a pixel lookup via the layer expression’s built in sampleImage function, reporting the value back as a text value that can be read in by JSX.
    </p>
    <p>
        JSX cannot be linked with other libraries or frameworks, so we were unable to try any loop-level parallelism.  
    </p>
    </div>

    <div class=block>
    <div class=subtitle id=experiments-paper-expand>Experiments [Paper Expand]</div>

    <p>
        We parameterize the filter with the following values:
        <ul>
            <li>Points: number of points. Can change with remeshing step</li>
            <li>Iterations: number of iterations Step Forward</li>
            <li>Frameskip: paper effect is recomputed every x frames</li>
            <li>Radius: the 'thickness' of the paper around the subject</li>
        </ul>
    </p>
    <p>
        Script v4 introduces the remeshing step as well as point re-use between frames, while script v1 initializes and optimizes a new point list each iteration. In benchmarking performance (Tab 1), we find that increasing the number of points does not affect v4 as much, mainly due to point re-use. Increasing the resolution also does not affect performance for both scripts since step sizes are proportional to composition area. Interestingly enough, changing the radius does not affect performance much for both scripts. This is because the sampleImage function used to access memory supports a radius parameter, so we can still execute calls to our paper extend effect with different radius parameters and still make the same number of calls to sampleImage. Calling sampleImage with a radius of 1 or a radius of 100 does not make much of a performance difference to After Effects, and it shows in our results. 
    </p>

    <img src="assets/paper_table.png">
    <div class=caption>Tab 1. Performance of paper cutout averaged over 5 trials</div>

    <img src="assets/spiderman_01.gif">
    <div class=caption>Res 1. Hobbes With paper overlay #1 | Across the Spiderverse (2023)</div>
    <img src="assets/spiderman_02.gif">
    <div class=caption>Res 2. Hobbes With paper overlay #2 | Across the Spiderverse (2023)</div>

    <p>
        We ran our effect on several pieces of footage, two from Spiderverse (Res 1. & 2.) and two live-action (Res 3. & 4.). We rotoscoped the object of interest in After Effects and ran the v4 script. In most cases, we found that reducing the framerate to 12fps (animating on 2s) produced more interesting stylized results.
    </p>
    
    <img src="assets/dancing.gif">
    <div class=caption>Res 3. Full-body rapid movement subjects</div>

        For (Res 4.) we experimented with how the paper extend effect can be used in the VFX pipeline. By combining it with color-burn grunge textures, hue offsets, and exaggerated edge filters all confined to the paper bounds, we are able to create even more stylistic effects on real-life footage that better replicate the SpiderVerse NPR effect.

    <img src="assets/suit.gif">
    <div class=caption>Res 4. Paper expand [Left] and full composite [Right]</div>

    </div>

    <div class=block>
    <div class=subtitle id=#smearing>Approach [Frame Smearing]</div>

    <img src="assets/frame_smearing/spiderverse_smear_example.jpeg">
    <div class=caption>Fig 1. Frame Smearing Example | Across the Spiderverse (2023)</div>

    <p>
        The frame smearing plugin aims to create an artistic "smearing" effect which interpolates consecutive frames of movement and creates more movement in the composition. This is better illustrated in the example taken from Spiderverse. There are multiple approaches to achieve this.
    </p>
    <p>
        For one, the shader would need to keep track of the key features that are moving across frames. Then, we would need to extract those key features across a set of consecutive frames, and render them on the current frame. Given that we are limited to 2D footage, this task becomes challenging, as we have no access to 3D vertices that we can extract displacement information from. Moreover, since we can’t pass information between frames in the AE plugin, we decide not to obtain the smearing effect by extracting the moving features from following frames. 
    </p>
    
    <img src="assets/frame_smearing/test_frame_smear_1.gif">
    <img src="assets/frame_smearing/test_frame_smear_1_dense.gif">
    <div class=caption>Fig 2. Optical Flow Map Example | Tom and Jerry (2022)</div>

    <p>
        Instead, we employ a simplified approach using optical flow. Our workflow is segmented into two steps. First, we run the input video through our optical flow map generator script. This script utilizes the OpenCV API to perform dense optical flow calculations (Gunnar-Farneback method), obtaining velocity vectors illustrating movement between frames. This script produces a new video of the same length, depicting velocity vectors between each frame in HSV, where hue represents the angle of direction and value represents the magnitude of direction. Above is an example of the optical flow map video. We also employ a combination of gaussian and median filtering to denoise this output video. 
    </p>
    <p>
        Next, we insert this generated optical flow map video as an additional layer in After Effects to be inputted into the plugin script (analogous to passing a second texture to the shader). The shader in the plugin script will determine the magnitude and direction of motion at any given pixel (given from the optical flow texture) and will subsequently perform an averaging of pixels in the magnitude and direction of motion. The algorithm looks like the following: 
    </p>
    <img src="assets/frame_smearing/smear_algo.png">
    <img src="assets/frame_smearing/smear_ae_parameters.png">
    <div class=caption>Fig 2. Algorithm and AE Parameter Sliders</div>
        
    <p>
        Given a pixel's velocity vector's magnitude (derived from optical flow map) is over a threshold constant, we perform averaging over a number of sample points, each point obtained by solving the equation of motion: (x_2, y_2) = (x_1, y_2) s * v * t, where s denotes the scale parameter, v denotes the direction vector obtained from the optical flow map, and t denotes the reciprocal of the number of samples. We parameterize the scale of motion, number of samples, and velocity threshold into AE sliders such that compositors can easily change and tune the effect to their liking. Number of samples determines how many times we observe the smear lines. Scale determines how spaced apart the smear lines are. Threshold determines over which minimum velocity should smear lines be rendered. For instance, setting threshold at 1 would produce no motion blur, while setting threshold to 0 would produce all motion blur as indicated by the optical flow texture. 
    </p>
    <p>
        Fortunately, this shader algorithm runs in real-time and responds instantly to changes in parameterization (for which we cap each parameter at a fixed range for real-time performance), which is what we aim for. This way, artists can preview their composites as soon as they add the effect. We also demonstrate that this plugin can be used over any footage, animation or real-world. Below is a gallery of our carefully-tuned motion blur composites.  
    </p>
    </div>



    
    <div class=block>
    <div class=subtitle id=#dithering>Approach [Dithering]</div>

    <img src="assets/dither.gif">
    <div class=caption>Fig 1. Introduction of Hobbes | Across the Spiderverse (2023)</div>

    <p>
        The dithering script uses a fixed Bayer 3x3 matrix for each frame and off-colors each pixel region (currently each 3x3 pixel square in a frame through modulation) to create a dithering pattern. This process is memoryless and does not use previous frames to inform current frames. The severity of the coloring can be increased or decreased by modifying an levels variable which essentially alters the number of colors available for that region's palette, moving pixel colorations to the nearest available option after applying the dithering matrix. At a high levels value, the produced frames of the video become visually identical to the original, while low level values produce more distinguishable differentiated dithering regions.
    </p>
    </div>

    <div class=block>
        <div class=subtitle id=#finalevaluation>Final Evaluation</div>

        <img src="assets/final.gif">
        <div class=caption>Fig 1. Compositing all filters | Across the Spiderverse (2023)</div>

        <p>
            For the final evaluation, we felt that because this is a creative product, that it requires creative evaluation. We interviewed people in the animation industry, from students to graphics engineers to animators, on their thoughts when viewing our results. Some feedback we recieved:
            <br><br>
            <b>Graphics Engineer</b>: looks cool, would love to see more about the robustness.<br>
            <b>After Effects User #1</b>: really cool, can we ship some of these effects in After Effects as a plugin?<br>
            <b>After Effects User #2</b>: I love how quick and easy it is to pick up these effects. The beauty is in the parameteribility<br>

        </p>
    </div>



    
    <div class=block>
        <div class=subtitle id=responsibilities>Responsibilities</div>
        <p>
            Oscar Dadfar (odadfar):
            <ul>
                <li>Wrote "Paper Expand" as an After Effects script</li>
                <li>Ran "Paper Expand" tests and VFX compositing example</li>
                <li>Prepared After Effects layer sampling plugin code for starting VFX shaders for teammates</li>
                <li>Ported RGB glitch shader to After Effects as a plugin</li>
                <li>Wrote website template (what you're seeing here)</li>
            </ul>
        </p>
        <p>
            Olivia Loh (olivia77):
            <ul>
                <li>Wrote "Frame Smear" as an After Effects plugin</li>
                <li>Wrote "Motion Blur" as an After Effects plugin</li>
                <li>Wrote optical flow layer generator for "Frame Smear" plugin</li> 
                <li>Ran "Frame Smear" and "Motion Blur" tests and VFX compositing example</li>
            </ul>
        </p>
        <p>
            Maxton Huff (maxton):
            <ul>
                <li>Wrote dithering shader script in Shadertoy</li>
                <li>Wrote Mr. Negative shader script in Shadertoy</li>
                <li>Wrote RGB glitch shader script in Shadertoy</li>
            </ul>
        </p>
    </div>

</body>
</html>
